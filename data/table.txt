
# x86 Instruction encoding table.

# Format:
# -----------------------------------------------------------------------------
# <mnemonic> <prefixes..> <opcode> <ops..> <flags..> <ctrs..>

# <mnemonic>  -  specifies the full name of the instruction, including prefixes
# and  suffixes such as "v-" for AVX instructions, "b" for byte operations etc.
# <prefixes..>  - specifies a list of prefixes.
# <opcode..>  -  specifies  an  instruction  opcode  in  lowercase  hexadecimal
# notation. 
# <ops..>  -  the immediate and address operands.
# <ctrs..>  -  specifies the list of constraints related to when an instruction
# needs  to  be decoded, such as "only decode when REX.W is set" or "when VEX.W
# is 1".
# <fields..> - specifies the list of fields set for an instruction, such as the
# direction bit, the wideness bit, 

# Mnemonics
# -----------------------------------------------------------------------------
# The  mnemonics are always a full instruction name in lowercase ascii. To keep
# the table properly aligned the mnemonics are truncated at 11 characters.
# In case the mnemonic name is larger than 11 character use the following table
# to figure out the full name of the mnemonic:
#    aeskeygen..  => aeskeygenassist
#    vaeskeyge..  => vaeskeygenassist
#    vpbc..i128   => vbroadcasti128
# These  long mnemonics are rare but kind of a pain for this table to deal with
# without assuming this expansion.

# Instruction prefixes
# -----------------------------------------------------------------------------
# The below lists group prefixes into categories.
# Extension prefixes:
#   vp - specifies the VEX prefix (bytes C4 and C5 followed by other VEX crap)
#   ep - specifies the EVEX prefix (byte 62 followed by EVEX crap)
# Mandatory "legacy" prefixes
#   np - no prefix.
#   66 - the operand size override prefix.
#   f3 - the rep(nz) prefix.
#   f2 - the repz prefix.
# Opcode escapes
#   0f - Two-byte opcode escape
#   3a - Three-byte opcode escape (in these tables always follows 0f)
#   3f - Three-byte opcode escape (in these tables always follows 0f)

# Opcode
# -----------------------------------------------------------------------------
# The  instruction  opcode  is  a  single byte in lowercase hexadecimal format.
# Opcode  can  be  extended by Mod/RM byte's rx field, in which case opcode has
# the  form  similar  to 80/1 for opcode for the SUB instruction. Sometimes the
# opcode   is extended by Mod/RM byte's mod field or VEX.W field, in which case
# that information is put into the flags array.
# Opcode  can  be  specified as a range by register set. An example is B8 (mov)
# that can be written as B8+reg.
# Opcode can imply the presence of a Mod/RM byte. An example is c6 (mov), which
# will  be  written  as c6/reg. The part after slash represents a register set,
# which are described below.

# Operands
# -----------------------------------------------------------------------------
# The  table  below  lists  operands  stored in bytes immediately following the
# opcodes.
# imm8 - an 8-bit immediate.
# imm16 - a 16-bit immediate.
# imm32 - a 32-bit immediate.
# immr - a 64-bit immediate (only `mov`).
# imm - an immediate whose size depends on the data size. In 64-bit mode it's
#   a 32-bit immediate.
# imm16imm8 - 16 bit immediate followed by 8-bit immediate (only `enter`)
# rel8 - an 8-bit short pointer (offset from RIP)
# rel16 - a 16-bit short pointer.
# rel32 - a 32-bit short pointer.
# rel - a short pointer whose size depends on the address size.
# far16 - a 16-bit far pointer (16-bit offset followed by 16-bit segment)
# far32 - a 32-bit far pointer (32-bit offset followed by 32-bit segment)
# far   - a far pointer (depends on address size)
# xmmimm - an xmm-register encoded in top 4 bits of an 8-bit immediate.

# Constraints
# -----------------------------------------------------------------------------
# The  constraints  are  prefixed  with  '@'  symbol and specify the additional
# conditions under which the particular decoding can be decoded.
#  @m64 - decode only in 64-bit mode.
#  @mcl - decode only in compatibility/legacy mode.
#  @ds64 - decode only if the operand size is 64-bit.
#  @ds32 - decode only if the operand size is 32-bit.
#  @ds16 - decode only if the operand size is 16-bit.
#  @dn64 - decode only if the operand size is not-64-bit.
#  @as64 - decode only if the address size is 64-bit.
#  @as32 - decode only if the address size is 32-bit.
#  @as16 - decode only if the address size is 16-bit.
#  @vexw1 - decode only if the VEX.W bit is set to 1.
#  @vexw0 - decode only if the VEX.W bit is set to 1.
#  @mod11 - decode only if the MOD field of the Mod/RM byte is 11
#  @moda - decode only if the MOD field of the Mod/RM byte is not 11
#  @modb - decode only if the MOD field of the Mod/RM byte is 01/10
#  @modab - decode only if the MOD field of the Mod/RM byte is not 00

# Fields
# -----------------------------------------------------------------------------
# The  fields  specify  additional information about an instruction that may be
# useful  for  disassemblers. The fields do not affect the process of decoding,
# but rather the process of interpretation.
#  +rx=<regset><number> - implicit register operand.
#  +rm=<regset> - specifies what set of registers RM indexes into in
#    case MOD field of the Mod/RM byte is 11. If this field is not specified,
#    the default is to assume RM is the same as RX.
#  +rxs=<size> - set rx operand size (also sets rm)
#  +rms=<size> - set rm operand size
#  +rmm - set only the memory size for rm operand (register same as rx)
#  +d - in a Mod/RM encoding RM byte is the destination. By default RX is
#    first.
#  +w - operand size is 8-bit (default depends on data size)
#  +ds64 - force 64-bit operand size, even in the absence of REX.W=1 field.
#  +ds32 - force 32-bit operand size
#  +ds16 - force 16-bit operand size, even in 32-bit mode.
#  +vexvz - VEX.vvvv is unused (=1111)
#  +far - the address operand given is a far pointer.
#  +rep - this instruction may have a rep prefix.

# Register sets
# -----------------------------------------------------------------------------
# reg - general purpose register (rax..r15, eax..r15d, ax..r15w, r0b..r15b)
# mmx - MMX registers (mm0 .. mm7)
# xmm - XMM/YMM registers (xmm0 .. xmm15, ymm0..ymm15)
# sreg - Segment register (ds, cs, ss, es, fs, gs)
# dreg - Debug register (dr0 .. dr7)
# creg - Control register (cr0 .. cr7)
# bndreg - BND Register
# st - x87 FPU register (st(0) .. st(7))

aam         d4 imm8 @mcl +rx=reg0
aad         d5 imm8 @mcl +rx=reg0
aaa         37 @mcl
aas         3f @mcl

adc         10/reg +d +w
adc         11/reg +d
adc         12/reg +w
adc         13/reg
adc         14 imm8 +w +rx=reg0
adc         15 imm +rx=reg0
adc         80/2 imm8 +w
adc         81/2 imm
adc         83/2 imm8

adcx        66 0f 38 f6/reg

add         00/reg +d +w
add         01/reg +d
add         02/reg +w
add         03/reg
add         04 imm8 +w +rx=reg0
add         05 imm +rx=reg0
add         80/0 imm8 +w
add         81/0 imm
add         83/0 imm8

addps       np 0f 58/xmm
addpd       66 0f 58/xmm
addss       f3 0f 58/xmm
addsd       f2 0f 58/xmm
vaddps      vp np 0f 58/xmm
vaddpd      vp 66 0f 58/xmm
vaddss      vp f3 0f 58/xmm
vaddsd      vp f2 0f 58/xmm

addsubpd    66 0f d0/xmm
addsubps    f2 0f d0/xmm
vaddsubpd   vp 66 0f d0/xmm
vaddsubps   vp f2 0f d0/xmm

adox        f3 0f 38 f6/reg

aesimc      66 0f 38 db/xmm
aesenc      66 0f 38 dc/xmm
aesenclast  66 0f 38 dd/xmm
aesdec      66 0f 38 de/xmm
aesdeclast  66 0f 38 df/xmm
aeskeygen.. 66 0f 3a df/xmm
vaesimc     vp 66 0f 38 db/xmm +vexvz
vaesenc     vp 66 0f 38 dc/xmm
vaesenclast vp 66 0f 38 dd/xmm
vaesdec     vp 66 0f 38 de/xmm
vaesdeclast vp 66 0f 38 df/xmm
vaeskeyge.. vp 66 0f 3a df/xmm +vexvz

and         20/reg +d +w
and         21/reg +d
and         22/reg +w
and         23/reg
and         24 imm8 +rx=reg0 +w
and         25 imm  +rx=reg0
and         80/4 imm8 +w
and         81/4 imm
and         83/4 imm8

andps       np 0f 54/xmm
andpd       66 0f 54/xmm
andps       vp np 0f 54/xmm
andpd       vp 66 0f 54/xmm

andnps      np 0f 55/xmm
andnpd      66 0f 55/xmm
andnps      vp np 0f 55/xmm
andnpd      vp 66 0f 55/xmm

arpl        63/reg @mcl +ds16

blendps     66 0f 3a 0c/xmm imm8
blendpd     66 0f 3a 0d/xmm xmmimm
vblendps    vp 66 0f 3a 0c/xmm xmmimm
vblendpd    vp 66 0f 3a 0d/xmm xmmimm

blendvps    66 0f 38 14/xmm +rx=xmm0
blendvpd    66 0f 38 15/xmm +rx=xmm0
vblendvps   vp 66 0f 3a 4a/xmm xmmimm @vexw0
vblendvpd   vp 66 0f 38 4b/xmm xmmimm @vexw0

bndcl       f3 0f 1a/reg @mcl +ds32
bndcl       f3 0f 1a/reg @m64 +ds64

bndldx      np 0f 1a/bndreg
bndmov      66 0f 1a/bndreg
bndcu       f2 0f 1a/bndreg
bndmov      66 0f 1b/bndreg +d
bndstx      np 0f 1b/bndreg +d
bndmk       f3 0f 1b/bndreg
bndcn       f2 0f 1b/bndreg

bound       62/reg

bsr         0f bd/reg
bsf         0f bc/reg
bswap       0f c8+reg

bt          0f a3/reg +d
bt          0f ba/4 imm8

btc         0f ba/7 imm8
btc         0f bb/reg +d

btr         0f b3/reg +d
btr         0f ba/6 imm8

bts         0f ab/reg +d
bts         0f ba/5 imm8

call        e8 rel
call        ff/2
call        9a far
call        ff/3 +far

cbw         98 @ds16
cwde        98 @ds32
cdqe        98 @ds64

cld         fc
clc         f8

clflush     np 0f ae/7 +w
clflushopt  66 0f ae/7

cli         fa
clts        0f 06
clui        f3 0f 01 ee
clwb        66 0f ae/6 +w
cmc         f5

cmovo       0f 40/reg
cmovno      0f 41/reg
cmovb       0f 42/reg
cmovae      0f 43/reg
cmovz       0f 44/reg
cmovnz      0f 45/reg
cmovbe      0f 46/reg
cmova       0f 47/reg
cmovs       0f 48/reg
cmovns      0f 49/reg
cmovp       0f 4a/reg
cmovnp      0f 4b/reg
cmovl       0f 4c/reg
cmovge      0f 4d/reg
cmovle      0f 4e/reg
cmovg       0f 4f/reg

cmp         38/reg +d +w
cmp         39/reg +d
cmp         3a/reg +w
cmp         3b/reg
cmp         3c imm8 +w +rx=reg0
cmp         3d imm +rx=reg0
cmp         80/7 imm8 +w
cmp         81/7 imm
cmp         83/7 imm8

cmpps       np 0f c2/xmm imm8
cmppd       66 0f c2/xmm imm8
cmpsd       f2 0f c2/xmm imm8
cmpss       f3 0f c2/xmm imm8
vcmpps      vp np 0f c2/xmm imm8
vcmppd      vp 66 0f c2/xmm imm8
vcmpsd      vp f2 0f c2/xmm imm8
vcmpss      vp f3 0f c2/xmm imm8

cmpxchg     0f b0/reg +w
cmpxchg     0f b1/reg

cmpxchg8b   0f c7/1 @dn64 @moda +ds64
# cmpxchg16b  0f c7/1 @ds64 @moda +ds128

cmpsb       a6 +rep
cmpsw       a7 @ds16 +rep
cmpsd       a7 @ds32 +rep
cmpsq       a7 @ds64 +rep

comiss      np 0f 2f/xmm
comisd      66 0f 2f/xmm
vcomiss     vp np 0f 2f/xmm +vexvz
vcomisd     vp 66 0f 2f/xmm +vexvz

cpuid       0f a2

crc32       f2 0f 38 f0/reg
crc32       f2 0f 38 f1/reg

cvtps2pd    np 0f 5a/xmm
cvtpd2ps    66 0f 5a/xmm
cvtss2sd    f3 0f 5a/xmm
cvtsd2ss    f2 0f 5a/xmm
vcvtps2pd   vp np 0f 5a/xmm +vexvz
vcvtpd2ps   vp 66 0f 5a/xmm +vexvz
vcvtss2sd   vp f3 0f 5a/xmm +vexvz
vcvtsd2ss   vp f2 0f 5a/xmm +vexvz

cvtdq2ps    np 0f 5b/xmm
cvtps2dq    66 0f 5b/xmm
cvttps2dq   f3 0f 5b/xmm
vcvtdq2ps   vp np 0f 5b/xmm +vexvz
vcvtps2dq   vp 66 0f 5b/xmm +vexvz
vcvttps2dq  vp f3 0f 5b/xmm +vexvz

cvttpd2dq   66 0f e6/xmm
cvtdq2pd    f3 0f e6/xmm
cvtpd2dq    f2 0f e6/xmm
vcvttpd2dq  vp 66 0f e6/xmm +vexvz
vcvtdq2pd   vp f3 0f e6/xmm +vexvz
vcvtpd2dq   vp f2 0f e6/xmm +vexvz

cvtpi2ps    np 0f 2a/xmm +rm=mmx
cvtpi2pd    66 0f 2a/xmm +rm=mmx
cvtsi2ss    f3 0f 2a/xmm +rm=reg
cvtsi2sd    f2 0f 2a/xmm +rm=reg
vcvtsi2ss   vp f3 0f 2a/xmm +rm=reg +vexvz
vcvtsd2si   vp f2 0f 2a/xmm +rm=reg +vexvz

cvtps2pi    np 0f 2d/mmx +rm=xmm
cvtpd2pi    66 0f 2d/mmx +rm=xmm
cvtss2si    f3 0f 2d/xmm +rm=reg
cvtsd2si    f2 0f 2d/reg +rm=xmm
vcvtss2si   vp f3 0f 2d/xmm +rm=reg +vexvz
vcvtsd2si   vp f2 0f 2d/reg +rm=xmm +vexvz

cvttps2pi   np 0f 2c/mmx +rm=xmm
cvttpd2pi   66 0f 2c/mmx +rm=xmm
cvttss2si   f3 0f 2c/reg +rm=xmm
cvttsd2si   f2 0f 2c/reg +rm=xmm
vcvttss2si  vp f3 0f 2c/reg +rm=xmm +vexvz
vcvttsd2si  vp f2 f2 0f 2c/reg +rm=xmm +vexvz

cwd         99 @ds16
cdq         99 @ds32
cqo         99 @ds64

daa         27 @mcl
das         2f @mcl

dec         fe/1 +w
dec         ff/1
dec         48+reg @mcl

div         f6/6 +w
div         f7/6

divps       np 0f 5e/xmm
divpd       66 0f 5e/xmm
divss       f3 0f 5e/xmm
divsd       f2 0f 5e/xmm
vdivps      vp np 0f 5e/xmm
vdivpd      vp 66 0f 5e/xmm
vdivss      vp f3 0f 5e/xmm
vdivsd      vp f2 0f 5e/xmm

dpps        66 0f 3a 40/xmm imm8
dppd        66 0f 3a 41/xmm imm8
vdpps       vp 66 0f 3a 40/xmm imm8
vdppd       vp 66 0f 3a 41/xmm imm8

emms        np 0f 77

endbr64     f3 0f 1e fa
endbr32     f3 0f 1e fb

enter       c8 imm16imm8

extractps   66 0f 3a 17/xmm +d +rm=reg
vextractps  vp 66 0f 3a 17/xmm +d +rm=reg

fxsave      np 0f ae/0
fxrstor     np 0f ae/1

haddpd      66 0f 7c/xmm
haddps      f2 0f 7c/xmm
vhaddpd     vp 66 0f 7c/xmm
vhaddps     vp f2 0f 7c/xmm

hlt         f4

hsubpd      66 0f 7d/xmm
hsubps      f2 0f 7d/xmm
vhsubpd     vp 66 0f 7d/xmm
vhsubps     vp f2 0f 7d/xmm

idiv        f6/7 +w
idiv        f7/7

imul        f6/5 +w
imul        f7/5
imul        0f af/reg
imul        6b/reg imm8
imul        69/reg imm

in          e4 imm8 +w +rx=reg0
in          e5 imm8 +rx=reg0
in          ec +w +rx=reg0
in          ed +rx=reg0

inc         fe/0 +w
inc         ff/0
inc         40+reg @mcl

insb        6c +ds8 +rep
insw        6d @ds16 +rep
insd        6d @ds32 +rep
insq        6d @ds64 +rep

insertps    66 0f 3a 21/xmm imm8
insertps    vp 66 0f 3a 21/xmm imm8

int3        cc
int         cd imm8
into        ce
int1        f1

invd        0f 08
invlpg      0f 01/7 @moda

iret        cf @ds16
iretd       cf @ds32
iretq       cf @ds64

jo          70 rel8
jno         71 rel8
jb          72 rel8
jae         73 rel8
jz          74 rel8
jnz         75 rel8
jbe         76 rel8
ja          77 rel8
js          78 rel8
jns         79 rel8
jp          7a rel8
jnp         7b rel8
jl          7c rel8
jge         7d rel8
jle         7e rel8
jg          7f rel8

jcxz        e3 rel8 @mcl @as16
jecxz       e3 rel8 @as32
jrcxz       e3 rel8 @m64 @as64

jo          0f 80 rel
jno         0f 81 rel
jb          0f 82 rel
jae         0f 83 rel
jz          0f 84 rel
jnz         0f 85 rel
jbe         0f 86 rel
ja          0f 87 rel
js          0f 88 rel
jns         0f 89 rel
jp          0f 8a rel
jnp         0f 8b rel
jl          0f 8c rel
jge         0f 8d rel
jle         0f 8e rel
jg          0f 8f rel

jmp         eb rel8
jmp         e9 rel
jmp         ff/4
jmp         ea far
jmp         ff/5 +far

lahf        9f @mcl

lar         0f 02/reg

lddqu       f2 0f f0/xmm
vlddqu      vp f2 0f f0/xmm +vexvz

ldmxcsr     np 0f ae/2
vldmxcsr    vp np 0f ae/2 +vexvz

lds         c5/reg @mcl +far
les         c4/reg @mcl +far
lss         0f b2/reg +far
lfs         0f b4/reg +far
lgs         0f b5/reg +far

lea         8d/reg

leave       c9

# lfence np 0f ae e8

lgdt        0f 01/2
lidt        0f 01/3

lldt        0f 00/2 +ds16
lmsw        0f 01/6 +ds16

lodsb       ac +ds8 +rep
lodsw       ad @ds16 +rep
lodsd       ad @ds32 +rep
lodsq       ad @ds64 +rep

loop        e2 rel8
loopz       e1 rel8
loopnz      e0 rel8

lsl         0f 03/reg +ds16
ltr         0f 00/3 +ds16

lzcnt       f3 0f bd/reg

maskmovdqu  66 0f f7/xmm
maskmovq    np 0f f7/mmx
vmaskmovdqu vp 66 0f f7/xmm +vexvz

maxps       np 0f 5f/xmm
maxpd       66 0f 5f/xmm
maxss       f3 0f 5f/xmm
maxsd       f2 0f 5f/xmm
vmaxps      vp np 0f 5f/xmm
vmaxpd      vp 66 0f 5f/xmm
vmaxss      vp f3 0f 5f/xmm
vmaxsd      vp f2 0f 5f/xmm

mfence      np 0f ae f0

minps       np 0f 5d/xmm
minpd       66 0f 5d/xmm
minss       f3 0f 5d/xmm
minsd       f2 0f 5d/xmm
vminps      vp np 0f 5d/xmm
vminpd      vp 66 0f 5d/xmm
vminss      vp f3 0f 5d/xmm
vminsd      vp f2 0f 5d/xmm

monitor 0f 01 c3

mov         88/reg +d +w
mov         89/reg +d
mov         8a/reg +w
mov         8b/reg
mov         8c/sreg +d +rm=reg
mov         8e/sreg +rm=reg
mov         a0 rel8 +d +rx=reg0
mov         a1 rel +d +rx=reg0
mov         a2 rel8 +rx=reg0
mov         a3 rel +rx=reg0
mov         b0+reg imm8 +w
mov         b8+reg immr
mov         c6/0 imm8 +w
mov         c7/0 imm

mov         0f 20/reg +rm=creg
mov         0f 21/reg +rm=dreg
mov         0f 22/creg +rm=reg
mov         0f 23/dreg +rm=reg

movaps      np 0f 28/xmm
movapd      66 0f 28/xmm
vmovaps     vp np 0f 28/xmm +vexvz
vmovapd     vp 66 0f 28/xmm +vexvz

movaps      np 0f 29/xmm +d
movapd      66 0f 29/xmm +d
vmovaps     vp np 0f 29/xmm +vexvz +d
vmovapd     vp 66 0f 29/xmm +vexvz +d

movbe       0f 38 f0/reg
movbe       0f 38 f1/reg +d

movq        np 0f 6f/mmx
movq        np 0f 7f/mmx +d
movq        f3 0f 7e/xmm
movq        66 0f d6/xmm +d
vmovq       vp 66 0f 6e/xmm @vexw1 +vexvz
vmovq       vp 66 0f 7e/xmm @vexw1 +d +vexvz

movd        np 0f 6e/mmx
movd        np 0f 7e/mmx +d
movd        66 0f 6e/xmm
movd        66 0f 7e/xmm +d
vmovd       vp 66 0f 6e/xmm @vexw0 +vexvz
vmovd       vp 66 0f 7e/xmm @vexw0 +d +vexvz

movddup     f2 0f 12/xmm
vmovddup    vp f2 0f 12/xmm +vexvz

movdqa      66 0f 6f/xmm
movdqu      f3 0f 6f/xmm
vmovdqa     vp 66 0f 6f/xmm +vexvz
vmovdqu     vp f3 0f 6f/xmm +vexvz

movdqa      66 0f 7f/xmm +d
movdqu      f3 0f 7f/xmm +d
vmovdqa     vp 66 0f 7f/xmm +d +vexvz
vmovdqu     vp f3 0f 7f/xmm +d +vexvz

movdq2q     f2 0f d6/mmx +rm=xmm

movhlps     np 0f 12/xmm @mod11
vmovhlps    vp np 0f 12/xmm @mod11

movlps      np 0f 12/xmm @moda
movlpd      66 0f 12/xmm @moda
vmovlps     vp np 0f 12/xmm @moda
vmovlpd     vp 66 0f 12/xmm @moda

movlps      np 0f 13/xmm @moda +d +vexvz
movlpd      66 0f 13/xmm @moda +d +vexvz
vmovlpd     vp np 0f 13/xmm @moda +d +vexvz
vmovlpd     vp 66 0f 13/xmm @moda +d +vexvz

movlhps     np 0f 16/xmm @mod11
vmovlhps    vp np 0f 16/xmm @mod11

movhps      np 0f 16/xmm @moda +rm=reg
movhpd      66 0f 16/xmm @moda +rm=reg
vmovhps     vp np 0f 16/xmm @moda +rm=reg
vmovhpd     vp 66 0f 16/xmm @moda +rm=reg

movhps      np 0f 17/xmm +d @moda +rm=reg
movhpd      66 0f 17/xmm +d @moda +rm=reg
vmovhps     vp np 0f 17/xmm +d @moda +rm=reg +vexvz
vmovhpd     vp 66 0f 17/xmm +d @moda +rm=reg +vexvz

movmskps    np 0f 50/reg +rm=xmm
movmskpd    66 0f 50/reg +rm=xmm
vmovmskpd   vp np 0f 50/reg +rm=xmm +vexvz
vmovmskpd   vp 66 0f 50/reg +rm=xmm +vexvz

movntdqa    66 0f 38 2a/xmm
vmovntdqa   vp 66 0f 38 2a/xmm +vexvz

movntdq     66 0f 38 e7/xmm
vmovntdq    vp 66 0f 38 e7/xmm +vexvz

movnti      np 0f c3/reg +d

movntps     np 0f 2b/xmm +d
movntpd     66 0f 2b/xmm +d
vmovntps    vp np 0f 2b/xmm +d +vexvz
vmovntpd    vp 66 0f 2b/xmm +d +vexvz

movntq      np 0f e7/mmx @moda +d +ds64

movq2dq     f3 0f d6/xmm @moda

movsb       a4 +ds8 +rep
movsw       a5 @ds16 +rep
movsd       a5 @ds32 +rep
movsq       a5 @ds64 +rep

movss       f3 0f 10/xmm
movsd       f2 0f 10/xmm
vmovss      vp f3 0f 10/xmm
vmovsd      vp f2 0f 10/xmm

vmovss      f3 0f 11/xmm
movsd       f2 0f 11/xmm +d
vmovss      vp f3 0f 11/xmm +d +vexvz
vmovsd      vp f2 0f 11/xmm +d +vexvz

movshdup    f3 0f 16/xmm
vmovshdup   vp f3 0f 16/xmm +vexvz

movsldup    f3 0f 12/xmm
vmovsldup   vp f3 0f 12/xmm +vexvz

movzx       0f b6/reg
movzx       0f b7/reg
movsx       0f be/reg
movsx       0f bf/reg
movsxd      63/reg @m64

movups      np 0f 10/xmm
movupd      66 0f 10/xmm
vmovups     vp np 0f 10/xmm +vexvz
vmovupd     vp 66 0f 10/xmm +vexvz

movups      np 0f 11/xmm +d
movupd      66 0f 11/xmm +d
vmovups     vp np 0f 11/xmm +d +vexvz
vmovupd     vp 66 0f 11/xmm +d +vexvz

mpsadbw     66 0f 3a 42/xmm +imm8
vmpsadbw    vp 66 0f 3a 42/xmm +imm8

mul         f6/4 +w
mul         f7/4

mulps       np 0f 59/xmm
mulpd       66 0f 59/xmm
mulss       f3 0f 59/xmm
mulsd       f2 0f 59/xmm
vmulps      vp np 0f 59/xmm
vmulpd      vp 66 0f 59/xmm
vmulss      vp f3 0f 59/xmm
vmulsd      vp f2 0f 59/xmm

mwait       0f 01 c9

neg         f6/3 +w
neg         f7/3

# nop         90
nop         0f 1f/0

not         f6/2 +w
not         f7/2

or          0c imm8 +rx=reg0 +w
or          0d imm +rx=reg0
or          80/1 imm8 +w
or          81/1 imm
or          83/1 imm8
or          08/reg +d +w
or          09/reg +d
or          0a/reg +w
or          0b/reg

orps        np 0f 56/xmm
orpd        66 0f 56/xmm
vorps       vp np 0f 56/xmm
vorpd       vp 66 0f 56/xmm

out         e6 imm8 +w +rx=reg0
out         e7 imm8 +rx=reg0
out         ee +w
out         ef

outsb       6e +ds8 +rep
outsw       6f @ds16 +rep
outsd       6f @ds32 +rep
outsd       6f @ds64 +ds32 +rep

pabsb       np 0f 38 1c/mmx
pabsw       np 0f 38 1d/mmx
pabsd       np 0f 38 1e/mmx
pabsb       66 0f 38 1c/xmm
pabsw       66 0f 38 1d/xmm
pabsd       66 0f 38 1e/xmm
vpabsb      vp 66 0f 38 1c/xmm +vexvz
vpabsw      vp 66 0f 38 1d/xmm +vexvz
vpabsd      vp 66 0f 38 1e/xmm +vexvz

packsswb    np 0f 63/mmx
packssdw    np 0f 6b/mmx
packsswb    66 0f 63/xmm
packssdw    66 0f 6b/xmm
vpacksswb   vp 66 0f 63/xmm
vpackssdw   vp 66 0f 6b/xmm

packusdw    66 0f 38 2b/xmm
vpackusdw   vp 66 0f 38 2b/xmm

packuswb    np 0f 67/mmx
packuswb    66 0f 67/xmm
vpackuswb   vp 66 0f 67/xmm

paddb       np 0f fc/mmx
paddw       np 0f fd/mmx
paddd       np 0f fe/mmx
paddq       np 0f d4/mmx
paddb       66 0f fc/xmm
paddw       66 0f fd/xmm
paddd       66 0f fe/xmm
paddq       66 0f d4/xmm
vpaddb      vp 66 0f fc/xmm
vpaddw      vp 66 0f fd/xmm
vpaddd      vp 66 0f fe/xmm
vpaddq      vp 66 0f d4/xmm

paddsb      np 0f ec/mmx
paddsb      66 0f ec/xmm
paddsw      np 0f ed/mmx
paddsw      66 0f ed/xmm
vpaddsb     vp 66 0f ec/xmm
vpaddsw     vp 66 0f ed/xmm

paddusb     np 0f dc/mmx
paddusb     66 0f dc/xmm
vpaddusb    vp 66 0f dc/xmm
paddusw     np 0f dd/mmx
paddusb     66 0f dd/xmm
vpaddusb    vp 66 0f dd/xmm

palignr     np 0f 3a 0f/mmx
palignr     66 0f 3a 0f/xmm
vpalignr    vp 66 0f 3a 0f/xmm

pand        np 0f db/mmx
pand        66 0f db/xmm
vpand       vp 66 0f db/xmm

pandn       np 0f df/mmx
pandn       66 0f df/xmm
vpandn      vp 66 0f df/xmm

# pause f3 90

pavgb       np 0f e0/mmx
pavgb       66 0f e0/xmm
vpavgb      vp 66 0f e0/xmm
pavgb       np 0f e3/mmx
pavgb       66 0f e3/xmm
vpavgb      vp 66 0f e3/xmm

pblendvb    66 0f 38 10/xmm
vpblendbv   vp 66 0f 3a 4c xmmimm @vexw0

pblendw     66 0f 3a 0e/xmm imm8
vpblendw    vp 66 0f 3a 0e/xmm imm8

pclmuldq    66 0f 3a 44/xmm imm8
vpclmuldq   vp 66 0f 3a 44/xmm imm8

pcmpeqb     np 0f 74/mmx
pcmpeqb     66 0f 74/xmm
vpcmpeqb    vp 66 0f 74/xmm
pcmpeqw     np 0f 75/mmx
pcmpeqw     66 0f 75/xmm
vpcmpeqw    vp 66 0f 75/xmm
pcmpeqd     np 0f 76/mmx
pcmpeqd     66 0f 76/xmm
vpcmpeqd    vp 66 0f 76/xmm

pcmpeqq     66 0f 38 29/xmm
vpcmpeqq    vp 66 0f 38 29/xmm

pcmpestrm   66 0f 3a 60/xmm imm8
pcmpestri   66 0f 3a 61/xmm imm8
vpcmpestrm  vp 66 0f 3a 60/xmm imm8
vpcmpestri  vp 66 0f 3a 61/xmm imm8

pcmpgtb     np 0f 64/mmx
pcmpgtb     66 0f 64/xmm
vpcmpgtb    vp 66 0f 64/xmm
pcmpgtw     np 0f 65/mmx
pcmpgtw     66 0f 65/xmm
vpcmpgtw    vp 66 0f 65/xmm
pcmpgtd     np 0f 66/mmx
pcmpgtd     66 0f 66/xmm
vpcmpgtd    vp 66 0f 66/xmm
pcmpgtq     66 0f 38 37/xmm
vpcmpgtq    vp 66 0f 38 37/xmm

pcmpistrm   66 0f 3a 62/xmm imm8
pcmpistri   66 0f 3a 63/xmm imm8
vpcmpistrm  vp 66 0f 3a 62/xmm imm8
vpcmpistri  vp 66 0f 3a 63/xmm imm8

pextrb      66 0f 3a 14/xmm imm8 +w +d +rm=reg
pextrd      66 0f 3a 16/xmm imm8 +rm=reg
pextrq      66 0f 3a 16/xmm imm8 @ds64 +rm=reg
vpextrb     vp 66 0f 3a 14/xmm imm8 @vexw0 +vexvz +w +d +rm=reg
vpextrd     vp 66 0f 3a 16/xmm imm8 @vexw0 +vexvz +rm=reg
vpextrq     vp 66 0f 3a 16/xmm imm8 @vexw1 @ds64 +vexvz +rm=reg

pextrw      np 0f c5/mmx imm8
pextrw      66 0f c5/xmm imm8
pextrw      66 0f 3a 15/xmm imm8 +ds16
vpextrw     vp 66 0f c5/xmm imm8 @vexw0 +vexvz
vpextrw     vp 66 0f 3a 15/xmm imm8 @vexw0 +vexvz +ds16

phaddw      np 0f 38 01/mmx +ds16
phaddw      np 0f 38 02/mmx +ds32
phaddw      66 0f 38 01/xmm +ds16
phaddw      66 0f 38 02/xmm +ds32
vphaddw     vp 66 0f 38 01/xmm +ds16
vphaddw     vp 66 0f 38 02/xmm +ds32

phaddsw     np 0f 38 03/mmx +ds16
phaddsw     66 0f 38 03/xmm +ds16
vphaddsw    vp 66 0f 38 03/xmm +ds16

phminposuw  66 0f 38 41/xmm
vphminposuw vp 66 0f 38 41/xmm +vexvz

phsubw      np 0f 38 05/mmx +ds16
phsubw      np 0f 38 06/mmx +ds32
phsubw      66 0f 38 05/xmm +ds16
phsubw      66 0f 38 06/xmm +ds32
vphsubw     vp 66 0f 38 05/xmm +ds16
vphsubw     vp 66 0f 38 06/xmm +ds32

phsubsw     np 0f 38 07/mmx +ds16
phsubsw     66 0f 38 07/xmm +ds16
vphsubsw    vp 66 0f 38 07/xmm +ds16

pinsrb      66 0f 3a 20/xmm imm8 +rm=reg +w
pinsrd      66 0f 3a 22/xmm imm8 @dn64 +rm=reg
pinsrq      66 0f 3a 22/xmm imm8 @ds64 +rm=reg
vpinsrb     vp 66 0f 3a 20/xmm imm8 @vexw0 +rm=reg +w
vpinsrd     vp 66 0f 3a 22/xmm imm8 @vexw0 +rm=reg
vpinsrq     vp 66 0f 3a 22/xmm imm8 @vexw1 +rm=reg

pinsrw      np 0f c4/mmx imm8 +rm=reg
pinsrw      66 0f c4/xmm imm8 +rm=reg
vpinsrw     vp 66 0f c4/xmm imm8 +rm=reg

pmaddubsw   np 0f 38 04/mmx
pmaddubsw   66 0f 38 04/xmm
vpmaddubsw  vp 66 0f 38 04/xmm

pmaddwd     np 0f f5/mmx
pmaddwd     66 0f f5/xmm
pmaddwd     vp 66 0f f5/xmm

pmaxsw      np 0f ee/mmx
pmaxsw      66 0f ee/xmm
pmaxsb      66 0f 38 3c/xmm
pmaxsd      66 0f 38 3d/xmm
vpmaxsw     vp 66 0f ee/xmm
vpmaxsb     vp 66 0f 38 3c/xmm
vpmaxsd     vp 66 0f 38 3d/xmm

pmaxub      np 0f de/mmx
pmaxub      66 0f de/xmm
pmaxuw      66 0f 38 3e/xmm
pmaxud      66 0f 38 3f/xmm
vpmaxub     vp 66 0f de/xmm
vpmaxuw     vp 66 0f 38 3e/xmm
vpmaxud     vp 66 0f 38 3f/xmm

pminsw      np 0f ea/mmx
pminsb      66 0f 38 38/mmx
pminsd      66 0f 38 39/xmm
pminsw      66 0f ea/xmm
vpminsb     vp 66 0f 38 38/mmx
vpminsd     vp 66 0f 38 39/xmm
vpminsw     vp 66 0f ea/xmm

pminub      np 0f da/mmx
pminub      66 0f da/xmm
pminuw      66 0f 38 3a/xmm
vpminub     vp 66 0f da/xmm
vpminuw     vp 66 0f 38 3a/xmm

pminud      66 0f 38 3b/xmm
vpminud     vp 66 0f 38 3b/xmm

pmovmskb    np 0f d7/mmx
pmovmskb    66 0f d7/xmm

pmovsxbw    66 0f 38 20/xmm +ds64
pmovsxbd    66 0f 38 21/xmm +ds32
pmovsxbq    66 0f 38 22/xmm +ds16
pmovsxwd    66 0f 38 23/xmm +ds64
pmovsxwq    66 0f 38 24/xmm +ds32
pmovsxdq    66 0f 38 25/xmm +ds64
vpmovsxbw   vp 66 0f 38 20/xmm +ds64 +vexvz
vpmovsxbd   vp 66 0f 38 21/xmm +ds32 +vexvz
vpmovsxbq   vp 66 0f 38 22/xmm +ds16 +vexvz
vpmovsxwd   vp 66 0f 38 23/xmm +ds64 +vexvz
vpmovsxwq   vp 66 0f 38 24/xmm +ds32 +vexvz
vpmovsxdq   vp 66 0f 38 25/xmm +ds64 +vexvz

pmovzxbw    66 0f 38 30/xmm +ds64
pmovzxbd    66 0f 38 31/xmm +ds32
pmovzxbq    66 0f 38 32/xmm +ds16
pmovzxwd    66 0f 38 33/xmm +ds64
pmovzxwq    66 0f 38 34/xmm +ds32
pmovzxdq    66 0f 38 35/xmm +ds64
vpmovzxbw   vp 66 0f 38 30/xmm +ds64 +vexvz
vpmovzxbd   vp 66 0f 38 31/xmm +ds32 +vexvz
vpmovzxbq   vp 66 0f 38 32/xmm +ds16 +vexvz
vpmovzxwd   vp 66 0f 38 33/xmm +ds64 +vexvz
vpmovzxwq   vp 66 0f 38 34/xmm +ds32 +vexvz
vpmovzxdq   vp 66 0f 38 35/xmm +ds64 +vexvz

pmuldq      66 0f 38 28/xmm
vpmuldq     vp 66 0f 38 28/xmm

pmulhrsw    np 0f 38 0b/mmx
pmulhrsw    66 0f 38 0b/xmm
vpmulhrsw   vp 66 0f 38 0b/xmm

pmulhuw     np 0f e4/mmx
pmulhuw     66 0f e4/xmm
vpmulhuw    vp 66 0f e4/xmm

pmulhw      np 0f e5/mmx
pmulhw      66 0f e5/xmm
vpmulhw     vp 66 0f e5/xmm

pmulld      66 0f 38 40/xmm
vpmulld     vp 66 0f 38 40/xmm

pmullw      np 0f d5/mmx
pmullw      66 0f d5/xmm
vpmullw     vp 66 0f d5/xmm

pmuludq     np 0f f4/mmx
pmuludq     66 0f f4/xmm
pmuludq     vp 66 0f f4/xmm

pop         8f/0 @mcl
pop         8f/0 @m64 +ds64
pop         58+reg @mcl
pop         58+reg @m64 +ds64
pop         07 +rx=sreg0
pop         17 +rx=sreg2
pop         1f +rx=sreg3
pop         0f a1 +rx=sreg4
pop         0f a9 +rx=sreg5

popa        61 @as16
popad       61 @as32

popcnt      f3 0f b8/reg

popf        9d @as16
popfd       9d @as32
popfq       9d @as64

por         np 0f eb/mmx
por         66 0f eb/xmm
vpor        vp 66 0f eb/xmm

prefetcht0   0f 18/1 @moda +w
prefetcht1   0f 18/2 @moda +w
prefetcht2   0f 18/3 @moda +w
prefetchtnta 0f 18/0 @moda +w
prefetchw    0f 0d/1 @moda +w

psadbw       np 0f f6/mmx
psadbw       66 0f f6/xmm
vpsadbw      vp 66 0f f6/xmm

pshufb       np 0f 38 00/mmx
pshufb       66 0f 38 00/xmm
vpshufb      vp 66 0f 38 00/xmm

pshufw       np 0f 70/mmx imm8
pshufd       66 0f 70/xmm imm8
pshufhw      f3 0f 70/xmm imm8
pshuflw      f2 0f 70/xmm imm8
vpshufd      vp 66 0f 70/xmm imm8 +vexvz
vpshufhw     vp f3 0f 70/xmm imm8 +vexvz
vpshuflw     vp f2 0f 70/xmm imm8 +vexvz

psignb       np 0f 38 08/mmx
psignw       np 0f 38 09/mmx
psignd       np 0f 38 0a/mmx
psignb       66 0f 38 08/xmm
psignw       66 0f 38 09/xmm
psignd       66 0f 38 0a/xmm
vpsignb      vp 66 0f 38 08/xmm
vpsignw      vp 66 0f 38 09/xmm
vpsignd      vp 66 0f 38 0a/xmm

pslldq       66 0f 73/7 +rm=xmm imm8
vpslldq      vp 66 0f 73/7 +rm=xmm imm8

psllw        np 0f f1/mmx
pslld        np 0f f2/mmx
psllq        np 0f f3/mmx
psllw        np 0f 71/6 imm8 +rm=mmx
pslld        np 0f 72/6 imm8 +rm=mmx
psllq        np 0f 73/6 imm8 +rm=mmx

psllw        66 0f f1/xmm
pslld        66 0f f2/xmm
psllq        66 0f f3/xmm
psllw        66 0f 71/6 imm8 +rm=xmm
pslld        66 0f 72/6 imm8 +rm=xmm
psllq        66 0f 73/6 imm8 +rm=xmm

vpsllw       vp 66 0f f1/xmm
vpslld       vp 66 0f f2/xmm
vpsllq       vp 66 0f f3/xmm
vpsllw       vp 66 0f 71/6 imm8 +rm=xmm
vpslld       vp 66 0f 72/6 imm8 +rm=xmm
vpsllq       vp 66 0f 73/6 imm8 +rm=xmm

psraw        np 0f e1/mmx
psrad        np 0f e2/mmx
psraw        np 0f 71/4 imm8 +rm=mmx
psrad        np 0f 72/4 imm8 +rm=mmx

psraw        66 0f e1/xmm
psrad        66 0f e2/xmm
psraw        66 0f 71/4 imm8 +rm=xmm
psrad        66 0f 72/4 imm8 +rm=xmm

vpsraw       vp 66 0f e1/xmm
vpsrad       vp 66 0f e2/xmm
vpsraw       vp 66 0f 71/4 imm8 +rm=xmm
vpsrad       vp 66 0f 72/4 imm8 +rm=xmm

psrldq       66 0f 73/3 imm8 +d
vpsrldq      vp 66 0f 73/3 imm8 +d

psrlw        np 0f d1/mmx
psrld        np 0f d2/mmx
psrlq        np 0f d3/mmx
psrlw        np 0f 71/2 imm8 +rm=mmx
psrld        np 0f 72/2 imm8 +rm=mmx

psrlw        66 0f d1/xmm
psrld        66 0f d2/xmm
psrlq        66 0f d3/xmm
psrlw        66 0f 71/2 imm8 +rm=xmm
psrld        66 0f 72/2 imm8 +rm=xmm

vpsrlw       vp 66 0f d1/xmm
vpsrld       vp 66 0f d2/xmm
vpsrlq       vp 66 0f d3/xmm
vpsrlw       vp 66 0f 71/2 imm8 +rm=xmm
vpsrld       vp 66 0f 72/2 imm8 +rm=xmm

psubb        np 0f f8/mmx
psubw        np 0f f9/mmx
psubd        np 0f fa/mmx
psubq        np 0f fb/mmx
psubb        66 0f f8/xmm
psubw        66 0f f9/xmm
psubd        66 0f fa/xmm
psubq        66 0f fb/xmm
vpsubb       vp 66 0f f8/xmm
vpsubw       vp 66 0f f9/xmm
vpsubd       vp 66 0f fa/xmm
vpsubq       vp 66 0f fb/xmm

psubsb       np 0f e8/mmx
psubsw       np 0f e9/mmx
psubsb       66 0f e8/xmm
psubsw       66 0f e9/xmm
vpsubsb      vp 66 0f e8/xmm
vpsubsw      vp 66 0f e9/xmm

psubusb      np 0f d8/mmx
psubusw      np 0f d9/mmx
psubusb      66 0f d8/xmm
psubusw      66 0f d9/xmm
vpsubusb     vp 66 0f d8/xmm
vpsubusw     vp 66 0f d9/xmm

ptest        66 0f 38 17/xmm
vptest       vp 66 0f 38 17/xmm

ptwrite      f3 0f ae/4 @m64
ptwrite      f3 0f ae/4 @mcl +ds32

punpckldq    np 0f 62/mmx
punpckhbw    np 0f 68/mmx
punpckhwd    np 0f 69/mmx
punpckhdq    np 0f 6a/mmx
punpckhqdq   np 0f 6d/mmx
punpckldq    66 0f 62/xmm
punpckhbw    66 0f 68/xmm
punpckhwd    66 0f 69/xmm
punpckhdq    66 0f 6a/xmm
punpcklqdq   66 0f 6c/xmm
punpckhqdq   66 0f 6d/xmm
vpunpckldq   vp 66 0f 62/xmm
punpckhbw    vp 66 0f 68/xmm
punpckhwd    vp 66 0f 69/xmm
punpckhdq    vp 66 0f 6a/xmm
vpunpcklqdq  vp 66 0f 6c/xmm
punpckhqdq   vp 66 0f 6d/xmm

push         ff/6 @mcl
push         ff/6 @m64 +ds64
push         50+reg @mcl
push         50+reg @m64 +ds64
push         6a imm8
push         68 imm
push         06 +rx=sreg0
push         0e +rx=sreg1
push         16 +rx=sreg2
push         1e +rx=sreg3
push         0f a0 +rx=sreg4
push         0f a8 +rx=sreg5

pusha        60 @mcl @ds16
pushad       60 @mcl @ds32

pushf        9c @ds16
pushf        9c @mcl @ds32
pushf        9c @m64

pxor         np 0f ef/mmx
pxor         66 0f ef/xmm
vpxor        vp 66 0f ef/xmm

rol          c0/0 +w imm8
rol          c1/0 imm8
rol          d0/0 +w
rol          d1/0
rol          d2/0 +w +rx=reg1
rol          d3/0 +rx=reg1

ror          c0/1 +w imm8
ror          c1/1 imm8
ror          d0/1 +w
ror          d1/1
ror          d2/1 +w +rx=reg1
ror          d3/1 +rx=reg1

rcl          c0/2 +w imm8
rcl          c1/2 imm8
rcl          d0/2 +w
rcl          d1/2
rcl          d2/2 +w +rx=reg1
rcl          d3/2 +rx=reg1

rcr          c0/3 +w imm8
rcr          c1/3 imm8
rcr          d0/3 +w
rcr          d1/3
rcr          d2/3 +w +rx=reg1
rcr          d3/3 +rx=reg1

sal          c0/4 +w imm8
sal          c1/4 imm8
sal          d0/4 +w
sal          d1/4
sal          d2/4 +w +rx=reg1
sal          d3/4 +rx=reg1

shr          c0/5 +w imm8
shr          c1/5 imm8
shr          d0/5 +w
shr          d1/5
shr          d2/5 +w +rx=reg1
shr          d3/5 +rx=reg1

sar          c0/7 +w imm8
sar          c1/7 imm8
sar          d0/7 +w
sar          d1/7
sar          d2/7 +w +rx=reg1
sar          d3/7 +rx=reg1

rcpps        np 0f 53/xmm
rcpss        f3 0f 53/xmm
vrcpps       vp np 0f 53/xmm
vrcpss       vp f3 0f 53/xmm

rdfsbase     f3 0f ae/0
rdgsbase     f3 0f ae/1

rdmsr        0f 32

rdpid        f3 0f c7/7

# rdpkru       np 0f 01 ee

rdpmc        0f 33

# rdrand       NFx 0f c7/6
# rdseed       NFx 0f c7/7

rdtsc        0f 31

# rdtscp       0f 01 f9

ret          c3
ret          cb +far
ret          c2 imm16
ret          ca imm16 +far

# rorx         vp f2 0f 3a/reg imm8 @vexw0

roundps      66 0f 3a 08/xmm imm8
roundpd      66 0f 3a 09/xmm imm8
roundss      66 0f 3a 0a/xmm imm8
roundsd      66 0f 3a 0b/xmm imm8
vroundps     vp 66 0f 3a 08/xmm imm8 +vexvz
vroundpd     vp 66 0f 3a 09/xmm imm8 +vexvz
vroundss     vp 66 0f 3a 0a/xmm imm8 +vexvz
vroundsd     vp 66 0f 3a 0b/xmm imm8 +vexvz

rsm          0f aa

rsqrtps      np 0f 52/xmm
rsqrtss      f3 0f 52/xmm
vrsqrtps     vp np 0f 52/xmm +vexvz
vrsqrtss     vp f3 0f 52/xmm +vexvz

# rstorssp     f3 0f 01/5 @moda

sahf         9e @mcl

sbb          1c imm8 +w
sbb          1d imm
sbb          80/3 imm8 +w
sbb          81/3 imm
sbb          83/3 imm8
sbb          18/reg +d +w
sbb          19/reg +d
sbb          1a/reg +w
sbb          1b/reg

scasb        ae +ds8 +rep
scasw        ae @ds16 +rep
scasd        ae @ds32 +rep
scasq        ae @ds64 +rep

seto         0f 90 rel8
setno        0f 91 rel8
setb         0f 92 rel8
setae        0f 93 rel8
setz         0f 94 rel8
setnz        0f 95 rel8
setbe        0f 96 rel8
seta         0f 97 rel8
sets         0f 98 rel8
setns        0f 99 rel8
setp         0f 9a rel8
setnp        0f 9b rel8
setl         0f 9c rel8
setge        0f 9d rel8
setle        0f 9e rel8
setg         0f 9f rel8

sgdt         0f 01/0 @moda

shld         0f a4/reg +d imm8
shld         0f a5/reg +d

shrd         0f ac/reg +d imm8
shrd         0f ad/reg +d

shufps       np 0f c6/xmm imm8
shufpd       66 0f c6/xmm imm8
vshufps      vp np 0f c6/xmm imm8
vshufpd      vp 66 0f c6/xmm imm8

sidt         0f 01/1 @moda
sldt         0f 00/0 @moda +ds16
smsw         0f 01/4 @moda

sqrtps       np 0f 51/xmm
sqrtpd       66 0f 51/xmm
sqrtss       f3 0f 51/xmm
sqrtsd       f2 0f 51/xmm
vsqrtps      vp np 0f 51/xmm
vsqrtpd      vp 66 0f 51/xmm
vsqrtss      vp f3 0f 51/xmm
vsqrtsd      vp f2 0f 51/xmm

std          fd
sti          fb

stmxcsr      np 0f ae/3 @moda
vstmxcsr     vp np 0f ae/3 @moda +vexvz

stosb        aa +ds8 +rep
stosw        ab @ds16 +rep
stosd        ab @ds32 +rep
stosq        ab @ds64 +rep

str          0f 00/1 @moda

sub          2c imm8 +w +rx=reg0
sub          2d imm +rx=reg0
sub          80/5 imm8 +w
sub          81/5 imm
sub          83/5 imm8
sub          28/reg +d +w
sub          29/reg +d
sub          2a/reg +w
sub          2b/reg

subps        np 0f 5c/xmm
subpd        66 0f 5c/xmm
subss        f3 0f 5c/xmm
subsd        f2 0f 5c/xmm
vsubps       vp np 0f 5c/xmm
vsubpd       vp 66 0f 5c/xmm
vsubss       vp f3 0f 5c/xmm
vsubsd       vp f2 0f 5c/xmm

# swapgs       0f 01 f8

syscall      0f 05 @m64
sysret       0f 07 @m64
sysenter     0f 34
sysexit      0f 35

test         a8 imm8 +w +rx=reg0
test         a9 imm +rx=reg0
test         f6/0 imm8 +w
test         f7/0 imm
test         84/reg +d +w
test         85/reg +d

tzcnt        f3 0f bc/reg

ucomiss      np 0f 2e/xmm
ucomisd      66 0f 2e/xmm
vucomiss     vp np 0f 2e/xmm +vexvz
vucomisd     vp 66 0f 2e/xmm +vexvz

ud0          0f ff/reg
ud1          0f b9/reg
ud2          0f 0b

umonitor     f3 0f ae/6

unpckhps     np 0f 15/xmm
unpckhpd     66 0f 15/xmm
vunpckhps    vp np 0f 15/xmm
vunpckhpd    vp 66 0f 15/xmm

unpcklps     np 0f 14/xmm
unpcklpd     66 0f 14/xmm
vunpcklps    vp np 0f 14/xmm
vunpcklpd    vp 66 0f 14/xmm

vbroadcastss vp 66 0f 38 18/xmm @vexw0 +vexvz
vbroadcastsd vp 66 0f 38 19/xmm @vexw0 +vexvz

verr         0f 00/4
verw         0f 00/5

vextractf128 vp 66 0f 3a 19/xmm imm8 @vexw0
vextracti128 vp 66 0f 3a 39/xmm imm8 @vexw0

vinsertf128  vp 66 0f 3a 18/xmm imm8 @vexw0

vmaskmovps   vp 66 0f 38 2c/xmm @vexw0
vmaskmovpd   vp 66 0f 38 2d/xmm @vexw0
vmaskmovps   vp 66 0f 38 2e/xmm @vexw0 +d
vmaskmovpd   vp 66 0f 38 2f/xmm @vexw0 +d

vpblendd     vp 66 0f 3a 02/xmm imm8 @vexw0

vpbroadcastb vp 66 0f 38 78/xmm imm8 @vexw0
vpbroadcastw vp 66 0f 38 79/xmm imm8 @vexw0
vpbroadcastd vp 66 0f 38 58/xmm imm8 @vexw0
vpbroadcastd vp 66 0f 38 59/xmm imm8 @vexw0
vpbc..i128   vp 66 0f 38 5a/xmm imm8 @vexw0

vperm2f128   vp 66 0f 3a 06/xmm imm8 @vexw0
vperm2i128   vp 66 0f 3a 46/xmm imm8 @vexw0
vpermd       vp 66 0f 38 36/xmm @vexw0
vpermilps    vp 66 0f 38 0c/xmm @vexw0
vpermilpd    vp 66 0f 38 0d/xmm @vexw0
vpermilps    vp 66 0f 38 04/xmm imm8 @vexw0
vpermilpd    vp 66 0f 38 05/xmm imm8 @vexw0
vpermpd      vp 66 0f 3a 01/xmm imm8 @vexw1
vpermps      vp 66 0f 38 16/xmm imm8 @vexw0
vpermq       vp 66 0f 3a 00/xmm imm8 @vexw1

vpmaskmovd   vp 66 0f 38 8c/xmm @vexw0
vpmaskmovq   vp 66 0f 38 8c/xmm @vexw1
vpmaskmovd   vp 66 0f 38 8e/xmm @vexw0 +d
vpmaskmovq   vp 66 0f 38 8e/xmm @vexw1 +d

vpsllvd      vp 66 0f 38 47/xmm @vexw0
vpsllvq      vp 66 0f 38 47/xmm @vexw1

vpsravd      vp 66 0f 38 46/xmm @vexw0

vpsrlvd      vp 66 0f 38 45/xmm @vexw0
vpsrlvq      vp 66 0f 38 45/xmm @vexw1

vtestpd      vp 66 0f 38 0e/xmm @vexw0 +vexvz
vtestpd      vp 66 0f 38 0f/xmm @vexw0 +vexvz

vzeroall     vp np 0f 77 @vexw0
vzeroupper   vp np 0f 77 @vexw1

wbinvd      0f 09
wbnoinvd    f3 0f 09

wrfsbase    f3 0f ae/2 @mod11
wrgsbase    f3 0f ae/3 @mod11
wrmsr       0f 30
wrpkru      np 0f 01 ef

xadd        0f c0/reg +d +w
xadd        0f c1/reg +d

xchg        90+reg +rx=reg0
xchg        86/reg +w
xchg        87/reg

# xgetbv      np 0f 01 d0

xlatb       d7

xor         34 imm8 +rx=reg0 +w
xor         35 imm +rx=reg0
xor         80/6 imm8 +w
xor         81/6 imm
xor         83/6 imm8
xor         30/reg +d +w
xor         31/reg +d
xor         32/reg +w
xor         33/reg

xorps       np 0f 57/xmm
xorpd       66 0f 57/xmm
vxorps      vp np 0f 57/xmm
vxorpd      vp 66 0f 57/xmm

xsave       np 0f ae/4 @dn64 @moda
xsave64     np 0f ae/4 @ds64 @moda
xrstor      np 0f ae/5 @dn64 @moda
xrstor64    np 0f ae/5 @ds64 @moda
